{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QhWnsXWkgpD"
      },
      "source": [
        "# 第7章: 単語ベクトル\n",
        "単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwngqpeFkpJf"
      },
      "source": [
        "## 60. 単語ベクトルの読み込みと表示\n",
        "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/u/1/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nJsxPMQkt8l"
      },
      "source": [
        "## 61. 単語の類似度\n",
        "“United States”と”U.S.”のコサイン類似度を計算せよ．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXIt1gJkxFv"
      },
      "source": [
        "## 62. 類似度の高い単語10件サイン類似度が高い10語と，その類似度を出力せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGJSU-HnkzYs"
      },
      "source": [
        "## 63. 加法構成性によるアナロジー\n",
        "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWA6X7WXk2Tr"
      },
      "source": [
        "## 64. アナロジーデータでの実験\n",
        "単語アナロジーの評価データをダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaVoXvVNo4wd",
        "outputId": "76bf8c2b-ef6a-4cd2-c442-6997102adf2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-06-09 07:19:27--  http://download.tensorflow.org/data/questions-words.txt\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.126.128, 2a00:1450:4013:c01::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.126.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603955 (590K) [text/plain]\n",
            "Saving to: ‘questions-words.txt’\n",
            "\n",
            "questions-words.txt 100%[===================>] 589.80K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-06-09 07:19:27 (171 MB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## 評価データダウンロード\n",
        "!wget http://download.tensorflow.org/data/questions-words.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwjieCBpLJT"
      },
      "source": [
        "このデータは、(Athens-Greece, Tokyo-Japan)のように、意味的アナロジーを評価するための組と、(walk-walks, write-writes)のように文法的アナロジーを評価する組が含まれる．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVhcAgAxqcJn",
        "outputId": "c89bbb77-dd6f-49c1-e516-0af675e59dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": capital-common-countries\n",
            "Athens Greece Baghdad Iraq\n",
            "Athens Greece Bangkok Thailand\n",
            "Athens Greece Beijing China\n",
            "Athens Greece Berlin Germany\n"
          ]
        }
      ],
      "source": [
        "!head questions-words.txt -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be_aUKgDk4Th"
      },
      "source": [
        "## 65. アナロジータスクでの正解率\n",
        "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-HS-i5Gk6y7"
      },
      "source": [
        "## 66. WordSimilarity-353での評価\n",
        "[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_AFMOSbXWL",
        "outputId": "2c40dd0e-bc7f-4646-830b-3522bcdf180d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-06-09 11:00:13--  http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving www.gabrilovich.com (www.gabrilovich.com)... 208.97.177.37\n",
            "Connecting to www.gabrilovich.com (www.gabrilovich.com)|208.97.177.37|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23257 (23K) [application/zip]\n",
            "Saving to: ‘wordsim353.zip’\n",
            "\n",
            "wordsim353.zip      100%[===================>]  22.71K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-06-09 11:00:13 (269 KB/s) - ‘wordsim353.zip’ saved [23257/23257]\n",
            "\n",
            "Archive:  wordsim353.zip\n",
            "  inflating: combined.csv            \n",
            "  inflating: set1.csv                \n",
            "  inflating: set2.csv                \n",
            "  inflating: combined.tab            \n",
            "  inflating: set1.tab                \n",
            "  inflating: set2.tab                \n",
            "  inflating: instructions.txt        \n"
          ]
        }
      ],
      "source": [
        "!wget http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
        "!unzip wordsim353.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thK_8Z3obiC_",
        "outputId": "120de267-0c99-4453-cc19-831b75d6f574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word 1,Word 2,Human (mean)\r\n",
            "love,sex,6.77\r\n",
            "tiger,cat,7.35\r\n",
            "tiger,tiger,10.00\r\n",
            "book,paper,7.46\r\n",
            "computer,keyboard,7.62\r\n",
            "computer,internet,7.58\r\n",
            "plane,car,5.77\r\n",
            "train,car,6.31\r\n",
            "telephone,communication,7.50\r\n"
          ]
        }
      ],
      "source": [
        "!head  './combined.csv' -n 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGcTAvCmk9Ay"
      },
      "source": [
        "## 67. k-meansクラスタリング\n",
        "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOYVR3MXk_yQ"
      },
      "source": [
        "## 68. Ward法によるクラスタリング\n",
        "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn41cF2VlBcG"
      },
      "source": [
        "## 69. t-SNEによる可視化\n",
        "ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ．"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Chapter7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
